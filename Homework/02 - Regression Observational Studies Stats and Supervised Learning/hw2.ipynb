{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d0380f",
   "metadata": {},
   "source": [
    "# Homework 2 (HW2)\n",
    "By the end of this homework, we expect you to be able to:\n",
    "\n",
    "- Preprocess data and make it amenable to statistical analysis and machine learning models;\n",
    "- Train and test out-of-the-box machine learning models in Python;\n",
    "- Carry out simple multivariate regression analyses;\n",
    "- Use techniques to control for covariates;\n",
    "- Conduct an observational study and reason about its results.\n",
    "\n",
    "---\n",
    "\n",
    "- Homework release: Fri 17 Nov 2023\t\n",
    "\n",
    "- **Homework Due**: Fri 01 Dec 2023, 23:59\t\n",
    "\n",
    "- Grades released: Mon 11 Dec 2023\t\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Some rules\n",
    "1. You are allowed to use any built-in Python library that comes with Anaconda. If you want to use an external library, you may do so, but must justify your choice.\n",
    "\n",
    "2. Make sure you use the `data` folder provided in the repository in read-only mode. (Or alternatively, be sure you don’t change any of the files.)\n",
    "\n",
    "3. Be sure to provide a concise textual description of your thought process, the assumptions you made, the solution you implemented, and explanations for your answers. A notebook that only has code cells will not suffice.\n",
    "\n",
    "4. For questions containing the **/Discuss:/** prefix, answer not with code, but with a textual explanation **(in markdown)**.\n",
    "\n",
    "5. Back up any hypotheses and claims with data, since this is an important aspect of the course.\n",
    "\n",
    "6. Please write all your comments in **English**, and use meaningful variable names in your code. Your repo should have a single notebook (plus the required data files) in the master/main branch. **If there are multiple notebooks present, we will not grade anything.**\n",
    "\n",
    "7. We will **not run your notebook for you!** Rather, we will grade it as is, which means that only the results contained in your evaluated code cells will be considered, and we will not see the results in unevaluated code cells. Thus, be sure to hand in a **fully-run and evaluated notebook**. In order to check whether everything looks as intended, you can check the rendered notebook on the GitHub website once you have pushed your solution there.\n",
    "\n",
    "8. In continuation to the previous point, interactive plots, such as those generated using the `plotly` package, should be strictly avoided!\n",
    "\n",
    "9. Make sure to print results and/or dataframes that confirm you have properly addressed the task.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68937ccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T17:54:27.233757Z",
     "start_time": "2023-11-17T17:54:26.757026Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# regression / matching\n",
    "import statsmodels.formula.api as smf\n",
    "import networkx as nx\n",
    "\n",
    "# machine lerning\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb04befc",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "After two years, the EPFL Baseball Club is broke. The new Dean transferred all funds to EPFL's new poster child: its super-competitive Pétanque club. After struggling so much to learn about baseball, you have unfortunately been laid off...\n",
    "\n",
    "*(...) 1 month after, you manage to get another job (!) (...)*\n",
    "\n",
    "Congratulations! You have just been hired as a data scientist at the Association for Computational Linguistics (ACL), a professional organization for people working on natural language processing. The ACL organizes several of the top conferences and workshops in the field of computational linguistics and natural language processing.\n",
    "Your boss, Dr. Tiancheng, knows of your expertise in observational studies and asks you to investigate a question that’s been bothering everyone who has ever submitted a paper to a conference: should I spend time on writing rebuttals?\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Rebuttals, conferences, and getting your papers accepted\n",
    "\n",
    "Rebuttals in ACL (Association for Computational Linguistics) conferences and in many other academic conferences are an important part of the peer-review process. They allow authors of submitted papers to respond to the reviews and comments provided by the reviewers before a final decision is made regarding the acceptance of the paper. Here's how the rebuttal process typically works in ACL conferences:\n",
    "\n",
    "- Paper Submission: Authors submit their research papers to the ACL conference for review. These papers present novel research findings in computational linguistics, natural language processing, and related areas.\n",
    "- Peer Review: The papers undergo a peer-review process after the initial submission. The program committee or reviewers are experts in the field who evaluate the papers based on their quality, significance, novelty, methodology, and other relevant criteria. They provide comments and scores for each paper.\n",
    "- Rebuttal Period: After receiving the reviews, authors are given a specific period (usually around a week) to write a rebuttal. The rebuttal is a formal response to the reviewers' comments. It allows authors to clarify misunderstandings, address concerns, and provide additional information to support their paper's quality. \n",
    "- Final Review: After receiving the rebuttals, the reviewers may reconsider their initial assessments in light of the authors' responses. Reviewers may choose to maintain or adjust their reviews and scores based on the quality and effectiveness of the author's rebuttal.\n",
    "- Final Decision: The program committee or conference organizers consider the initial reviews/scores, rebuttals, and revised reviews/scores to make a final decision on the acceptance of the papers. The decision can be acceptance, rejection, or conditional acceptance with a request for revisions.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Data\n",
    "\n",
    "- `tmp_id`: Unique identifier for each paper in the format \"P{number}\".\n",
    "- `status`: Accept or Reject.\n",
    "- `submission_type`: Short vs. Long (papers can have different lengths). We do not use this column in this homework. \n",
    "- `track`: Track to which the paper was submitted, broadly speaking, the \"topic\" of the paper.\n",
    "- `scores_before`: Scores received before rebuttal. This is a nested JSON with many fields, but we will use only the \"overall\" score for the homework. \n",
    "- `scores_after`: Scores received after rebuttal. This is a nested JSON with many fields, but we will use only the \"overall\" score for the homework.\n",
    "- `had_rebuttal`: True or False.\n",
    "\n",
    "\n",
    "Note that: \n",
    " - reviews are assigned numbers, e.g., \"2\";\n",
    " - papers can have different numbers of reviews;\n",
    " - review numbers are arbitrary, e.g., `P1` in the dataframe has two reviews numbered \"2\" and \"3\" (but no review \"1\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28074a9c",
   "metadata": {},
   "source": [
    "## Task 1 (10 pts): Get to Know Your Data\n",
    "\n",
    "As a good data scientist, you first load the data and perform some small sanity checks on it.\n",
    "\n",
    "- You are expected to continuously alter your dataframe as you complete the tasks. E.g., if you are asked to filter the data in a specific task, continue using the filtered dataset in the subsequent tasks.\n",
    "- When we tell you to \"print the dataframe,\" make sure you print it in a way that shows the total number of rows and columns in it (`display(df)` should suffice)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ab063",
   "metadata": {},
   "source": [
    "**1.1** Load the dataset containing ACL reviews into memory using pandas. \n",
    "- For each paper, create rows `overall_score_before_avg` and `overall_score_after_avg` containing the average (overall) scores before and after rebuttal.\n",
    "- For each paper, create rows `overall_score_before_std` and `overall_score_after_std` containing the standard deviation of the overall scores before and after the rebuttal.\n",
    "- Print the four newly created rows for paper `P17`.\n",
    "- Print the resulting dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075ebff",
   "metadata": {},
   "source": [
    "**1.2** Create a single plot with 14 inches of width and 4 inches of height. The plot should contain two panels: \n",
    "- **Panel A**: The distribution of `overall_score_before_avg` for papers that were accepted and papers that were rejected.\n",
    "- **Panel B**: The distribution of `overall_score_before_avg` for papers that had rebuttals vs. papers that did not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea160c7",
   "metadata": {},
   "source": [
    "**1.3** **/Discuss/:** If you know a paper had a rebuttal, is it more or less likely that it was accepted? Does this mean that rebuttals help papers get accepted? Explain why or why not, providing a concrete example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87836f37",
   "metadata": {},
   "source": [
    "**1.4** Print the percentage of rebuttals per track in the conference (defined by the `track` column). \n",
    "\n",
    "**/Discuss:/** Using \"the logic\" of hypothesis testing (see slide 29 of Lecture 4), how would you devise a statistical test to refute the following null hypothesis: all tracks have the same fraction of papers with rebuttals. Your statistical test should consider all categories at once, rather than comparing the fraction of rebuttals between pairs of categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50275647",
   "metadata": {},
   "source": [
    "## Task 2 (10pts): Prediction\n",
    "\n",
    "You decide to investigate further the effect of rebuttals on acceptance using your machine learning skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578baafa",
   "metadata": {},
   "source": [
    "\n",
    "**2.1** For each possible value in the `track` column, create a new column called {track}-onehot (e.g., for track=Generation, create Generation-onehot). Collectively, these new columns should \"one hot-encode\" the track column---for instance, if for a given paper the `track` column is filled with the value \"Generation\", the Generation-onehot column should equal 1 and all other {track}-onehot columns should equal 0. \n",
    "\n",
    "Print the column names of the resulting dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aa41ea",
   "metadata": {},
   "source": [
    "\n",
    "**2.2** Create a column `had_rebuttal_int`, which equals 1 if the paper had a rebuttal, and 0 otherwise, and a column `accepted_int`, which equals 1 if the paper was accepted, and 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5806400",
   "metadata": {},
   "source": [
    "**2.3** Create a function `numpy_helper(df, cols)` to obtain a numpy.array out of your dataframe. The function should receive a dataframe `df` with N rows and a list of M columns `cols`, and should return a `np.array` of dimension `(NxM)` cast as a float."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dad455",
   "metadata": {},
   "source": [
    "\n",
    "**2.4**\n",
    "Create:\n",
    "- an array of features X containing all track one-hot features, as well as the `overall_score_before_avg`,`overall_score_before_std`, and `had_rebuttal_int`;\n",
    "- an array of outcomes y containing `accepted_int`. \n",
    "\n",
    "\n",
    "Print the shapes of both X and y (e.g., `X.shape`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf79a4",
   "metadata": {},
   "source": [
    "\n",
    "**2.5** Build two `GradientBoostingClassifier` models using `sklearn` using the default parameters:\n",
    "- Model 1: predicts the outcome `accepted_int` using the onehot encoded features related to track, as well as the `overall_score_before_avg`,`overall_score_before_std`.\n",
    "- Model 2:  predicts the outcome `accepted_int` using the onehot encoded features related to track, as well as the `overall_score_before_avg`,`overall_score_before_std` **and** `had_rebuttal_int`.\n",
    "\n",
    "\n",
    "For both models:\n",
    "\n",
    "- Use the `cross_validate` function from `sklearn.model_selection` to compute the average precision, recall, and accuracy across test cross validation splits.\n",
    "\n",
    "    - e.g., `cross_validate(clf, X, y, cv=30, scoring=('accuracy', 'precision', 'recall'))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abba6d4",
   "metadata": {},
   "source": [
    "\n",
    "**2.6** Determine whether the difference in accuracy of the two models is statistically significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518f2f6",
   "metadata": {},
   "source": [
    "**2.7** Contrast the results obtained in **2.6** with what you observed in **Task 1**. What advantage did the analyses in **2.6** have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52beb97e",
   "metadata": {},
   "source": [
    "## Task 3 (12pts): Interlude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f56eca",
   "metadata": {},
   "source": [
    "\n",
    "**3.1** Using the formula API from `statsmodels`, estimate the following linear regression. Report the summary of the models.\n",
    "- `accepted_int ~ had_rebuttal_int`,  \n",
    "- `accepted_int ~ overall_score_after_avg`\n",
    "- `had_rebuttal_int ~ overall_score_before_avg`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc5e23",
   "metadata": {},
   "source": [
    "\n",
    "**3.2** **/Discuss:/** Interpret the coefficients associated with the binary independent variables in the above models. Note that independent variables are the ones on the right-handside of the equation.\n",
    "\n",
    "- e.g., in `had_rebuttal_int ~ overall_score_before_avg`, `overall_score_before_avg` is the independent variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba81dc",
   "metadata": {},
   "source": [
    "\n",
    "**3.3** **/Discuss:/** describe three correlations you can draw from the previous analysis. Describe their sign (i.e., whether they are positive or negative), and whether they are statistically significant (at the .05 level of significance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59821c",
   "metadata": {},
   "source": [
    "**3.4** **/Discuss:/** Is the following statement True or False? Justify. \n",
    "\n",
    "- The variable `overall_score_after_avg` explains more of the variance in `accepted_int`than the variable `overall_score_before_avg` explains of `had_rebuttal_int`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b405fb7f",
   "metadata": {},
   "source": [
    "\n",
    "**3.5** **/Discuss:/** Create a causal diagram relating the following variables:\n",
    "- \"Sa\": `overall_score_after_avg`\n",
    "- \"Sb\": `overall_score_before_avg`\n",
    "- \"Re\": `had_rebuttal_int`\n",
    "- \"Ac\": `accepted_int`\n",
    "- \"Tr\": `track`\n",
    "\n",
    "\n",
    "When unsure about whether a causal relationship exists, include it in the diagram. E.g., include the arrow corresponding to the key questions around this homework, i.e., `had_rebuttal_int`->`accepted_int`, even though you are investigating whether it exists. \n",
    "\n",
    "You may draw your diagram using text, use Sa/Sb/Re/Ac/Tr to represent the names of the variables, and simply indicate the causal links, one per line.\n",
    "\n",
    "\n",
    "Instead of drawing something like this:\n",
    "![](./dagv.jpeg)\n",
    "\n",
    "Simply write:\n",
    "\n",
    "- Tr->Sb\n",
    "- Tr->Ac\n",
    "- Tr->Re\n",
    "- Ac->Sb\n",
    "- Re->Sb\n",
    "- Sb->Sa\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe89c00",
   "metadata": {},
   "source": [
    "**3.6** **/Discuss:/** What is the problem of simply comparing the outcomes of papers that had rebuttals with those that did not? Give a concrete example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ed68a4",
   "metadata": {},
   "source": [
    "# Task 4 (12 pts): Observational study\n",
    "\n",
    "You decide to use your observational study skills to obtain a concrete answer to the question: do rebuttals increase acceptance?\n",
    "\n",
    " **4.1** Perform exact one-to-one matching considering the `score_before_avg` and the `track` variables. Each paper that had a rebuttal (\"treatment group\") should be matched to a paper that did not have a rebuttal (\"control group\"). \n",
    "- Your matching should be optimal, i.e., the maximum amount of papers possible must be matched. \n",
    "- Print the dataframe of papers in the matched sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c97d1",
   "metadata": {},
   "source": [
    "**4.2** So far, we did not consider the `score_before_std` variable. One could argue that the variance in the scores makes a difference. E.g., a paper that received scores 1 and 5, might be very different from a paper with scores 3 and 3. \n",
    "\n",
    "Note that you did not match on the `score_before_std` variable. However, it suffices if this variable is \"balanced\" across treatment and control groups.\n",
    " Use the Standardized Mean Difference (SMD) to assess whether that's the case.\n",
    "\n",
    "- The standardized mean difference for a variable $x$ and two groups $t$ and $c$ is defined as: $\\frac{| E[x_t] - E[x_c] |}{\\sqrt{Var[x_t] + Var[x_c]}}$\n",
    "\n",
    "- Note that a Standardized Mean Difference smaller than 0.1 suggests that variables are balanced across treatment and control groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f788c",
   "metadata": {},
   "source": [
    "\n",
    "**4.3** Using the matched sample, estimate the following linear regression: `accepted ~ had_rebuttal_int`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8423e",
   "metadata": {},
   "source": [
    "\n",
    "**4.4** **/Discuss:/**\n",
    "\n",
    "i. Considering your results obtained in 4.3, and the causal diagram drawn in Task 3: do rebuttals increase the chance of a paper getting accepted? Why are results different from what you obtained in **Task 1?**\n",
    "\n",
    "ii. Why is there no need to include other covariates (e.g., score before) in the regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4867ea1",
   "metadata": {},
   "source": [
    "**4.5** **/Discuss:/** Imagine there is another, unobserved variable \"quality\" which captures the true quality of the paper. Suppose quality (\"Qu\") is connected to the DAG you drew in the following ways:\n",
    "- Qu -> Sa\n",
    "- Qu -> Sb\n",
    "- Qu -> Re\n",
    "- Qu -> Ac\n",
    "Assume that\n",
    "- quality can only increase the chances of rebuttals;\n",
    "- quality and the rebuttal can only increase the chance of a paper being accepted.\n",
    "Does this uncontrolled confounder threaten the validity of your findings?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
